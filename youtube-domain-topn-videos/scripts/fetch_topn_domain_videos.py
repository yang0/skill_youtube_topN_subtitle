#!/usr/bin/env python3
"""
Fetch top-N YouTube videos for a chosen domain in recent hours using Patchright.
"""

from __future__ import annotations

import argparse
import json
import re
import sys
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any
from urllib.parse import parse_qs, quote_plus, urljoin, urlparse

from patchright.sync_api import BrowserContext, Page, sync_playwright

TODAY_FILTER_SP = "EgQIAhAB"
SCRIPT_FILE = Path(__file__).resolve()
SKILL_ROOT = SCRIPT_FILE.parents[1]
PROJECT_ROOT = SCRIPT_FILE.parents[2]

DEFAULT_QUERY_TEMPLATES = [
    "{domain}",
    "{domain} news",
    "{domain} tutorial",
    "{domain} tools",
    "{domain} workflow",
    "{domain} case study",
    "{domain} best practices",
    "{domain} trend",
    "{domain} 入门",
    "{domain} 实战",
]

EXTRACT_ITEMS_JS = """
() => {
  const cards = Array.from(document.querySelectorAll("ytd-video-renderer"));
  return cards.map((card) => {
    const titleAnchor = card.querySelector("a#video-title");
    const title = (titleAnchor?.textContent || "").trim();
    const href = titleAnchor?.href || "";
    const descNode = card.querySelector("#description-text, yt-formatted-string#description-text");
    const description = (descNode?.textContent || "").trim();
    const channelNode = card.querySelector("#channel-name a, ytd-channel-name a");
    const channel = (channelNode?.textContent || "").trim();
    const metadata = Array.from(card.querySelectorAll("#metadata-line span"))
      .map((n) => (n.textContent || "").trim())
      .filter(Boolean);
    const viewsText = metadata[0] || "";
    const publishedText = metadata[1] || "";
    const durationNode = card.querySelector("ytd-thumbnail-overlay-time-status-renderer span");
    const duration = (durationNode?.textContent || "").trim();
    return { title, href, description, channel, viewsText, publishedText, duration };
  }).filter((x) => x.title && x.href);
}
"""

AGE_EN_PATTERN = re.compile(
    r"(?:streamed\s+)?(\d+)\s*(second|minute|hour|day|week|month|year)s?\s+ago",
    re.IGNORECASE,
)
AGE_ZH_PATTERN = re.compile(r"(\d+)\s*(秒|分钟|小时|天|周|个月|年)前")
VIEWS_EN_PATTERN = re.compile(r"([\d.,]+)\s*([kmb])?\s*views?", re.IGNORECASE)
VIEWS_ZH_PATTERN = re.compile(r"([\d.,]+)\s*([千萬万亿億])?\s*次观看")


@dataclass
class VideoItem:
    video_id: str
    title: str
    intro: str
    link: str
    channel: str
    views: int
    views_text: str
    published_text: str
    age_hours: float | None
    duration: str
    queries: list[str] = field(default_factory=list)


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description=(
            "Fetch top-N YouTube videos for a domain in recent hours using Patchright."
        )
    )
    parser.add_argument(
        "--domain",
        required=True,
        help="Domain/topic to search, for example 'robotics' or '跨境电商'.",
    )
    parser.add_argument("--top", type=int, default=10, help="Number of final ranked videos.")
    parser.add_argument("--hours", type=float, default=24.0, help="Maximum age (hours).")
    parser.add_argument(
        "--queries-file",
        default="",
        help="Explicit query list file (one query per line).",
    )
    parser.add_argument(
        "--query-templates-file",
        default="references/query_templates.txt",
        help="Query templates file (one template per line, supports {domain}).",
    )
    parser.add_argument(
        "--cookies-file",
        default="H:/cookies/youtube.txt",
        help="Cookie file path (Netscape or JSON). Use empty string to disable.",
    )
    parser.add_argument(
        "--max-results-per-query",
        type=int,
        default=25,
        help="Maximum candidate rows kept from each query page.",
    )
    parser.add_argument(
        "--scroll-rounds",
        type=int,
        default=5,
        help="How many times to scroll down for loading more cards.",
    )
    parser.add_argument(
        "--scroll-pause-ms",
        type=int,
        default=1200,
        help="Pause between scrolls in milliseconds.",
    )
    parser.add_argument("--gl", default="US", help="YouTube region parameter.")
    parser.add_argument("--hl", default="en", help="YouTube language parameter.")
    parser.add_argument("--locale", default="en-US", help="Browser locale.")
    parser.add_argument("--headed", action="store_true", help="Run browser in headed mode.")
    parser.add_argument(
        "--output-json",
        default="",
        help="Output JSON path. If empty, auto-generated by domain/top/hours.",
    )
    parser.add_argument(
        "--output-md",
        default="",
        help="Output markdown path. If empty, auto-generated by domain/top/hours.",
    )
    return parser.parse_args()


def load_nonempty_lines(path: Path) -> list[str]:
    if not path.exists():
        return []
    rows: list[str] = []
    for line in path.read_text(encoding="utf-8").splitlines():
        text = line.strip()
        if not text or text.startswith("#"):
            continue
        rows.append(text)
    return rows


def build_queries(domain: str, explicit_queries: list[str], templates: list[str]) -> list[str]:
    if explicit_queries:
        source = explicit_queries
    else:
        source = templates if templates else DEFAULT_QUERY_TEMPLATES

    built: list[str] = []
    for row in source:
        if "{domain}" in row:
            query = row.replace("{domain}", domain)
        elif explicit_queries:
            query = row
        else:
            query = f"{domain} {row}".strip()
        query = " ".join(query.split())
        if query:
            built.append(query)

    deduped: list[str] = []
    seen: set[str] = set()
    for item in built:
        key = item.lower()
        if key in seen:
            continue
        seen.add(key)
        deduped.append(item)
    return deduped


def sanitize_slug(text: str) -> str:
    slug = re.sub(r"[\\/:*?\"<>|\r\n\t]+", "-", text.strip().lower())
    slug = re.sub(r"\s+", "-", slug)
    slug = re.sub(r"-+", "-", slug).strip("-.")
    return slug or "domain"


def resolve_input_path(raw_path: str) -> Path:
    path = Path(raw_path).expanduser()
    if path.is_absolute():
        return path
    if path.exists():
        return path
    return SKILL_ROOT / path


def get_output_paths(args: argparse.Namespace) -> tuple[Path, Path]:
    if args.output_json and args.output_md:
        return Path(args.output_json), Path(args.output_md)

    now = datetime.now()
    date_dir = Path(str(now.year), f"{now.month:02d}", f"{now.day:02d}")
    date_label = now.strftime("%Y%m%d")
    base_dir = PROJECT_ROOT / "outputs" / date_dir

    hour_label = str(args.hours).replace(".", "p")
    stem = f"{date_label}_top{args.top}_{sanitize_slug(args.domain)}_{hour_label}h"
    json_path = Path(args.output_json) if args.output_json else (base_dir / f"{stem}.json")
    md_path = Path(args.output_md) if args.output_md else (base_dir / f"{stem}.md")
    return json_path, md_path


def normalize_video_url(raw_href: str) -> tuple[str, str]:
    full_url = urljoin("https://www.youtube.com", raw_href)
    parsed = urlparse(full_url)
    if parsed.path.startswith("/watch"):
        video_id = parse_qs(parsed.query).get("v", [""])[0]
        return f"https://www.youtube.com/watch?v={video_id}", video_id
    if parsed.path.startswith("/shorts/"):
        video_id = parsed.path.split("/shorts/", 1)[1].split("/", 1)[0]
        return f"https://www.youtube.com/watch?v={video_id}", video_id
    return full_url, ""


def parse_views(text: str) -> int:
    raw = text.strip()
    if not raw:
        return 0

    en_match = VIEWS_EN_PATTERN.search(raw.lower())
    if en_match:
        number = float(en_match.group(1).replace(",", ""))
        suffix = (en_match.group(2) or "").lower()
        multiplier = {"": 1, "k": 1_000, "m": 1_000_000, "b": 1_000_000_000}[suffix]
        return int(number * multiplier)

    zh_match = VIEWS_ZH_PATTERN.search(raw)
    if zh_match:
        number = float(zh_match.group(1).replace(",", ""))
        suffix = zh_match.group(2) or ""
        multiplier = {
            "": 1,
            "千": 1_000,
            "萬": 10_000,
            "万": 10_000,
            "億": 100_000_000,
            "亿": 100_000_000,
        }[suffix]
        return int(number * multiplier)

    plain_digits = re.search(r"([\d,]+)", raw)
    if plain_digits:
        return int(plain_digits.group(1).replace(",", ""))
    return 0


def parse_age_hours(text: str) -> float | None:
    raw = text.strip().lower()
    if not raw:
        return None
    if "live" in raw:
        return 0.0
    if "yesterday" in raw:
        return 24.0

    en_match = AGE_EN_PATTERN.search(raw)
    if en_match:
        value = float(en_match.group(1))
        unit = en_match.group(2).lower()
        factor = {
            "second": 1 / 3600,
            "minute": 1 / 60,
            "hour": 1,
            "day": 24,
            "week": 24 * 7,
            "month": 24 * 30,
            "year": 24 * 365,
        }[unit]
        return value * factor

    zh_match = AGE_ZH_PATTERN.search(text)
    if zh_match:
        value = float(zh_match.group(1))
        unit = zh_match.group(2)
        factor = {
            "秒": 1 / 3600,
            "分钟": 1 / 60,
            "小时": 1,
            "天": 24,
            "周": 24 * 7,
            "个月": 24 * 30,
            "年": 24 * 365,
        }[unit]
        return value * factor
    return None


def build_search_url(query: str, gl: str, hl: str) -> str:
    encoded_query = quote_plus(query)
    return (
        f"https://www.youtube.com/results?"
        f"search_query={encoded_query}&sp={TODAY_FILTER_SP}&gl={gl}&hl={hl}"
    )


def scrape_query(page: Page, query: str, args: argparse.Namespace) -> list[dict[str, Any]]:
    url = build_search_url(query, args.gl, args.hl)
    page.goto(url, wait_until="domcontentloaded", timeout=60_000)
    page.wait_for_timeout(2_000)
    for _ in range(args.scroll_rounds):
        page.mouse.wheel(0, 2500)
        page.wait_for_timeout(args.scroll_pause_ms)
    items = page.evaluate(EXTRACT_ITEMS_JS)
    if not isinstance(items, list):
        return []
    return items[: args.max_results_per_query]


def parse_cookie_file(cookie_path: Path) -> list[dict[str, Any]]:
    text = cookie_path.read_text(encoding="utf-8", errors="ignore")
    stripped = text.strip()
    if not stripped:
        return []

    if stripped.startswith("[") or stripped.startswith("{"):
        return parse_json_cookies(stripped)
    return parse_netscape_cookies(text)


def parse_json_cookies(raw_json: str) -> list[dict[str, Any]]:
    payload = json.loads(raw_json)
    if isinstance(payload, dict):
        source = payload.get("cookies", [])
    elif isinstance(payload, list):
        source = payload
    else:
        source = []

    cookies = []
    for entry in source:
        if not isinstance(entry, dict):
            continue
        name = entry.get("name")
        value = entry.get("value")
        domain = entry.get("domain") or entry.get("host")
        if not name or value is None or not domain:
            continue
        cookie: dict[str, Any] = {
            "name": str(name),
            "value": str(value),
            "domain": str(domain),
            "path": str(entry.get("path") or "/"),
            "secure": bool(entry.get("secure", False)),
            "httpOnly": bool(entry.get("httpOnly", False)),
        }
        same_site = entry.get("sameSite")
        if isinstance(same_site, str):
            normalized = same_site.capitalize()
            if normalized in {"Lax", "Strict", "None"}:
                cookie["sameSite"] = normalized

        expires = entry.get("expirationDate", entry.get("expires"))
        if isinstance(expires, (int, float)) and expires > 0:
            cookie["expires"] = int(expires)
        cookies.append(cookie)
    return cookies


def parse_netscape_cookies(raw_text: str) -> list[dict[str, Any]]:
    cookies: list[dict[str, Any]] = []
    for line in raw_text.splitlines():
        stripped = line.strip()
        if not stripped:
            continue

        http_only = False
        if stripped.startswith("#HttpOnly_"):
            stripped = stripped[len("#HttpOnly_") :]
            http_only = True
        elif stripped.startswith("#"):
            continue

        parts = stripped.split("\t")
        if len(parts) != 7:
            continue
        domain, _, path, secure_flag, expiry, name, value = parts
        if not domain or not name:
            continue
        cookie: dict[str, Any] = {
            "name": name,
            "value": value,
            "domain": domain,
            "path": path or "/",
            "secure": secure_flag.upper() == "TRUE",
            "httpOnly": http_only,
        }
        if expiry.isdigit():
            expires = int(expiry)
            if expires > 0:
                cookie["expires"] = expires
        cookies.append(cookie)
    return cookies


def apply_cookies(context: BrowserContext, cookie_file: str) -> int:
    if not cookie_file:
        return 0
    path = Path(cookie_file)
    if not path.exists():
        print(f"[WARN] Cookie file not found: {path}")
        return 0
    try:
        cookies = parse_cookie_file(path)
        if not cookies:
            print(f"[WARN] No valid cookies parsed from: {path}")
            return 0
        context.add_cookies(cookies)
        print(f"[INFO] Loaded {len(cookies)} cookies from {path}")
        return len(cookies)
    except Exception as exc:  # noqa: BLE001
        print(f"[WARN] Failed to load cookies: {exc}")
        return 0


def hot_score(item: VideoItem, max_hours: float) -> float:
    age = item.age_hours if item.age_hours is not None else max_hours
    return item.views / (age + 2.0)


def safe_md(text: str, limit: int = 120) -> str:
    compact = " ".join(text.split())
    trimmed = compact[:limit].rstrip()
    if len(compact) > limit:
        trimmed += "..."
    return trimmed.replace("|", "\\|")


def save_markdown(path: Path, rows: list[dict[str, Any]], domain: str, hours: float) -> None:
    lines = [
        f"# Top {len(rows)} Videos for '{domain}' ({hours}h)",
        "",
        "| Rank | Title | Intro | Data | Link |",
        "| --- | --- | --- | --- | --- |",
    ]
    for row in rows:
        data = row["data"]
        data_text = (
            f"domain={safe_md(data['domain'], 30)}; "
            f"views={data['views']} ({safe_md(data['views_text'], 40)}); "
            f"published={safe_md(data['published_text'], 40)}; "
            f"channel={safe_md(data['channel'], 40)}; "
            f"duration={safe_md(data['duration'], 20)}; "
            f"score={data['hot_score']}"
        )
        title = safe_md(row["title"], 80)
        intro = safe_md(row["intro"] or "(no intro)", 140)
        link = row["link"]
        lines.append(
            f"| {row['rank']} | {title} | {intro} | {data_text} | [watch]({link}) |"
        )
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text("\n".join(lines) + "\n", encoding="utf-8")


def main() -> int:
    args = parse_args()

    explicit_queries = (
        load_nonempty_lines(resolve_input_path(args.queries_file)) if args.queries_file else []
    )
    query_templates = load_nonempty_lines(resolve_input_path(args.query_templates_file))
    queries = build_queries(args.domain, explicit_queries, query_templates)
    if not queries:
        print("[ERROR] No queries available.")
        return 1

    json_path, md_path = get_output_paths(args)

    merged: dict[str, VideoItem] = {}
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(headless=not args.headed)
        context = browser.new_context(locale=args.locale, viewport={"width": 1440, "height": 2200})
        apply_cookies(context, args.cookies_file)
        page = context.new_page()

        for query in queries:
            print(f"[INFO] Query: {query}")
            try:
                rows = scrape_query(page, query, args)
            except Exception as exc:  # noqa: BLE001
                print(f"[WARN] Query failed: {query} ({exc})")
                continue

            for row in rows:
                raw_href = str(row.get("href", "")).strip()
                link, video_id = normalize_video_url(raw_href)
                if not video_id:
                    continue

                title = str(row.get("title", "")).strip()
                if not title:
                    continue

                intro = str(row.get("description", "")).strip()
                channel = str(row.get("channel", "")).strip()
                views_text = str(row.get("viewsText", "")).strip()
                published_text = str(row.get("publishedText", "")).strip()
                duration = str(row.get("duration", "")).strip()
                views = parse_views(views_text)
                age_hours = parse_age_hours(published_text)
                if age_hours is not None and age_hours > args.hours:
                    continue

                existing = merged.get(video_id)
                if existing:
                    if len(intro) > len(existing.intro):
                        existing.intro = intro
                    if views > existing.views:
                        existing.views = views
                        existing.views_text = views_text
                    if not existing.published_text and published_text:
                        existing.published_text = published_text
                    if existing.age_hours is None and age_hours is not None:
                        existing.age_hours = age_hours
                    if not existing.channel and channel:
                        existing.channel = channel
                    if not existing.duration and duration:
                        existing.duration = duration
                    if query not in existing.queries:
                        existing.queries.append(query)
                    continue

                merged[video_id] = VideoItem(
                    video_id=video_id,
                    title=title,
                    intro=intro,
                    link=link,
                    channel=channel,
                    views=views,
                    views_text=views_text,
                    published_text=published_text,
                    age_hours=age_hours,
                    duration=duration,
                    queries=[query],
                )

        context.close()
        browser.close()

    ranked = sorted(
        merged.values(),
        key=lambda item: (hot_score(item, args.hours), item.views),
        reverse=True,
    )[: args.top]

    if not ranked:
        print("[ERROR] No videos found. Try --headed or adjust queries.")
        return 1

    output_rows: list[dict[str, Any]] = []
    for index, item in enumerate(ranked, start=1):
        score = round(hot_score(item, args.hours), 2)
        row = {
            "rank": index,
            "title": item.title,
            "intro": item.intro,
            "data": {
                "domain": args.domain,
                "channel": item.channel,
                "views": item.views,
                "views_text": item.views_text,
                "published_text": item.published_text,
                "age_hours": None if item.age_hours is None else round(item.age_hours, 3),
                "duration": item.duration,
                "hot_score": score,
                "matched_queries": item.queries,
            },
            "link": item.link,
        }
        output_rows.append(row)
        print(
            f"[{index}] {item.title}\n"
            f"      {item.link}\n"
            f"      views={item.views} published='{item.published_text}' score={score}"
        )

    json_path.parent.mkdir(parents=True, exist_ok=True)
    json_path.write_text(json.dumps(output_rows, ensure_ascii=False, indent=2), encoding="utf-8")

    save_markdown(md_path, output_rows, args.domain, args.hours)

    print(f"[OK] JSON: {json_path.resolve()}")
    print(f"[OK] Markdown: {md_path.resolve()}")
    return 0


if __name__ == "__main__":
    sys.exit(main())
